# 2025-03-15

## Shader performance

Currently a lot of the "Modifier" nodes compute rotation matrices in the shader, even though they're static.
I think we should precompute those in JavaScript.

It's possible that the shader compilation step is doing this for me? Unsure, will see whether it is any more
performant afterwards.

Loading up the page on my laptop with the default object, it ends up at resolution scale 0.931.

But actually, this doesn't use any of Twist,LinearPattern,PolarPattern, which I think are the only ones
that compute `rotateToAxis()` in the shader.

With all 3 of those, it took 42 seconds (!) to compile the shader. And resolution scale settles at 0.608.

So now let's compute the matrices in JavaScript and pass them in.

It actually doesn't seem to have hardly helped at all? I wonder if the shader compiler has really effective constant folding?

Also, I may be able to define my "utility" functions in one shader file that I compile once, and then not recompile all
the same stuff every time when recompiling the user shader?

How can we do shader profiling? I am interested in profiling both compile time and runtime.

The next idea is if we can make union etc. check the bounding spheres and skip computations that can't affect the union?

Well I tried that but it also made performance worse??

I have reduced some branching in the shaders, doesn't seem to have helped much though.

I was curious if generating the shader code was slow, but it is like 0 ms to generate the shader code, almost all of the time is spent in compiling it.

I have found an improvement by breaking the ray marching loop sooner when we've travelled a long way (10x the camera distance from origin) and the distance
to the object is increasing (i.e. locally moving away from it), as a heuristic that we've gone past it.

For some reason the `SketchNode` shader is incredibly expensive to compile?

The default object takes about 1250 ms for me, which goes down to 405 ms if I
delete the sketch. So why is that slow?

## Interval arithmetic

I watched the Matt Keeter talk https://www.youtube.com/watch?v=UxGxsGnbyJ4 last night. I really like the idea of having the nodes return some intermediate
tree representation of their function, and then optimising that and turning it into GLSL code. That would also give a convenient way to evaluate the
nodes in JavaScript without needing them to provide 2 separate implementations. And it would give more opportunity to generate optimised GLSL.

If we pre-rotate the scene so that the camera is always pointing along the Z axis
(we may already have this condition?), then we could potentially use the interval arithmetic idea to
binary search the "ray" to work out where the surface is? Instead of naive ray marching. That would tell us if we shoot off to "infinity" (defined as
1000 let's say) in a single evaluation. And if the ray with Z from 0 to 1000 crosses the boundary, then calculate the ray from 0 to 500 and see if
that one crosses the boundary. And if not, calculate the ray from 250 to 500, and so on, until you are within 0.001 of the boundary, which will
be 20 steps.

We'd want our arithmetic implementation to have float/vec3/mat3/bool types, and
then just copy Matt Keeter's register allocation thing to turn it into
straight-line GLSL code. Potentially could even use/modify Fidget to do it?

One thing that concerns me is that the `SketchNode` shader code basically turns your
sketch into straight-line GLSL code that reuses the same variables over and over,
and it's super slow? What is up there?

## Gyroid

Gyroid is pretty cool and easy to do https://www.youtube.com/watch?v=gGwjJUD-OsI
